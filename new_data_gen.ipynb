{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cyclus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-637835de1b82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcymetric\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#import sklearn as skl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/cymetric/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcyclus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypesystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# only grabs code generated defintiions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcyclus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullBackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSqliteBack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHdf5Back\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mRecorder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cyclus'"
     ]
    }
   ],
   "source": [
    "import cymetric as cym\n",
    "import pandas as pd\n",
    "#import sklearn as skl\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (40,24)\n",
    "#from cymetric import graphs as cgr\n",
    "#from cymetric import timeseries as tm\n",
    "#from cymetric import filters as fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-c086b039809f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"simple/out-run7.py-.sqlite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"mass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cym' is not defined"
     ]
    }
   ],
   "source": [
    "file = \"simple/out-run7.py-.sqlite\"\n",
    "db = cym.dbopen(file)\n",
    "ev = cym.Evaluator(db=db, write=True)\n",
    "cym.graphs.flow_graph(ev, label = \"mass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract data from the sqlite file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_df(file):\n",
    "    db = cym.dbopen(file)\n",
    "    ev = cym.Evaluator(db=db, write=True)\n",
    "    \n",
    "    agentTable = ev.eval(\"AgentEntry\")\n",
    "    try: \n",
    "        agents = agentTable.loc[:, [\"AgentId\", \"Prototype\"]]\n",
    "    except: \n",
    "        print(\"there was an agent issue with \" + file)\n",
    "    \n",
    "    transTable = ev.eval(\"Transactions\")\n",
    "    try: \n",
    "        transactions = transTable.loc[:, [\"SenderId\", \"ReceiverId\", \"ResourceId\", \"Commodity\", \"Time\"]]\n",
    "    except: \n",
    "        print(\"there was an transactions issue with \" + file)\n",
    "    \n",
    "    resourceTable = ev.eval(\"Resources\")\n",
    "    try: \n",
    "        resources = resourceTable.loc[:, [\"ResourceId\", \"Quantity\"]]\n",
    "    except:\n",
    "        print(\"there was a resources issue with \" + file)\n",
    "    \n",
    "    #merge agents, transactions, and resources\n",
    "    int1 = pd.merge(transactions, resources, on='ResourceId', how='inner')\n",
    "\n",
    "    #rename AgentId column to facilitate merge \n",
    "    send = agents.rename(columns = {\"AgentId\": \"SenderId\"})\n",
    "    receive = agents.rename(columns = {\"AgentId\": \"ReceiverId\"})\n",
    "\n",
    "    for i in range(len(int1)):\n",
    "        for j in range(len(send)):\n",
    "            if int1.loc[i,\"SenderId\"] == send.loc[j, \"SenderId\"]:\n",
    "                int1.loc[i,\"SenderId\"] = send.loc[j, \"Prototype\"]\n",
    "                \n",
    "    for i in range(len(int1)):\n",
    "        for j in range(len(receive)):\n",
    "            if int1.loc[i,\"ReceiverId\"] == receive.loc[j, \"ReceiverId\"]:\n",
    "                int1.loc[i,\"ReceiverId\"] = receive.loc[j, \"Prototype\"]\n",
    "\n",
    "#     print(\"tidy_df output\")\n",
    "    print(int1)\n",
    "    return int1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trim resource-identifying columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_data(df):\n",
    "    trimmed = df[[\"SenderId\", \"ReceiverId\", \"Time\", \"Quantity\"]]\n",
    "    trimmed[\"fraction\"] = pd.Series(0, index = range(316)) #modular? \n",
    "    trimmed[\"truck\"] = pd.Series(0, index = range(316))\n",
    "    \n",
    "    #assume leu and heu enrichment happens in the same physical facility\n",
    "    #drop rows with transactions between enrichment facilities\n",
    "    short = trimmed.loc[(trimmed['SenderId'] != \"LEUenrich\") & (trimmed['ReceiverId'] != \"LEUtoHEUenrich\")]\n",
    "    \n",
    "    #change the name of enrichment facilites \n",
    "    short[\"SenderId\"] = short['SenderId'].replace({'LEUenrich': 'enrichment', 'LEUtoHEUenrich': 'enrichment'})\n",
    "    short['ReceiverId'] = short['ReceiverId'].replace({'LEUenrich': 'enrichment', 'LEUtoHEUenrich': 'enrichment'})\n",
    "    \n",
    "    return short"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define truck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for each transaction, cap shipment at a certain mass m ( = 25000kg? is realisitc, but a smaller value might be more interesting)\n",
    "create a new column with the number of trucks sent for this interaction (0 + )\n",
    "consider storing heu for several timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def send_trucks(truck_df, truck_size): \n",
    "    truck_df[\"fraction\"] = truck_df[\"Quantity\"] / truck_size #reevaluate this if you want the trucks to have different capacities\n",
    "    \n",
    "    #collect set of all transaction types: \n",
    "    transaction_pairs = Counter()\n",
    "    for i in range(len(truck_df)): \n",
    "        #add this transaction to the counter\n",
    "        #check if the value for that type of transaction is greater than the value for the truck \n",
    "        #if so, send a truck and update the value for the transaction type\n",
    "        #if not, update the value for the transaction type \n",
    "        #0: senderid, 1: receiverid, 2: time, 3: quantity, 4: fraction, 5: truck\n",
    "        sender = truck_df.iloc[i, 0]\n",
    "        receiver = truck_df.iloc[i, 1]\n",
    "        transaction_pairs.update({(sender, receiver): truck_df.iloc[i, 3]})\n",
    "        stored_material = transaction_pairs[(sender, receiver)]\n",
    "        if stored_material >= truck_size: \n",
    "            trucks = stored_material // truck_size\n",
    "            truck_df.at[i, \"truck\"] = trucks\n",
    "            transaction_pairs[(sender, receiver)] -= trucks * truck_size\n",
    "        else: \n",
    "            truck_df.at[i, \"truck\"] = 0\n",
    "        \n",
    "            \n",
    "    print(transaction_pairs)\n",
    "    print(truck_df)\n",
    "        \n",
    "    return truck_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### flatten data into single row for use with other runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns: every perumutation of transaction (truck boolean), every timestamp\n",
    "\n",
    "row: single cyclus run \n",
    "\n",
    "transactions: mine to enrichment, enrichment to reactor, reactor to sf sink, enrichment to heu sink\n",
    "\n",
    "#### modify this to look for its own facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   135/133m_t0  135/133_t0  135/131m_t0  133m/133_t0  133m/131m_t0  \\\n",
      "0          0.0         0.0          0.0          0.0           0.0   \n",
      "\n",
      "   133/131m_t0  135/133m_t1  135/133_t1  135/131m_t1  133m/133_t1  ...  \\\n",
      "0          0.0          0.0         0.0          0.0          0.0  ...   \n",
      "\n",
      "   135/131m_t142  133m/133_t142  133m/131m_t142  133/131m_t142  135/133m_t143  \\\n",
      "0   3.526805e+06       1.547456     5182.364967   46740.065614    9562.217039   \n",
      "\n",
      "   135/133_t143  135/131m_t143  133m/133_t143  133m/131m_t143  133/131m_t143  \n",
      "0   1061.364148   3.537151e+06       1.547928     5184.580595   46745.773549  \n",
      "\n",
      "[1 rows x 864 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def isotope_signal(): \n",
    "    #converted to seconds:\n",
    "    half131m = (11.9*24*60*60) #days\n",
    "    half133 = (5.25*24*60*60) #days\n",
    "    half133m = (2.19*24*60*60) #days\n",
    "    half135 = (9.10*60*60) #hours\n",
    "    time_step = (30*24*60*60)\n",
    "    \n",
    "    l131m = -np.log(2) / half131m\n",
    "    l133 = -np.log(2) / half133\n",
    "    l133m = -np.log(2) / half133m\n",
    "    l135 = -np.log(2) / half135\n",
    "\n",
    "    #find all of the reactor cycle starts \n",
    "    #calculate ratios for each t in cycle\n",
    "    isotope_rows = []\n",
    "    '''\n",
    "    for time step in time steps: \n",
    "        if before first delivery: \n",
    "            add only background (start with 0)\n",
    "            \n",
    "        calculate new ratios from previous time step\n",
    "        \n",
    "        if start of reactor cycle: \n",
    "            create new initial signal (n for each of the 4 isotopes)\n",
    "            calculate ratios\n",
    "            add to variable\n",
    "        \n",
    "        append ratios to row \n",
    "    '''\n",
    "    for i, t in enumerate(range(144)): \n",
    "        row = {\"135/133m\": 0, \"135/133\": 0, \"135/131m\": 0, \\\n",
    "               \"133m/133\": 0, \"133m/131m\": 0, \"133/131m\": 0}\n",
    "        if t >= 10: \n",
    "            previous_row = isotope_rows[i-1]\n",
    "            #how many half lives in 30 days? \n",
    "            #N(t) = N(0)/2 ** (t/thalf)\n",
    "            #Rn/m(t) = Rn/m(0)e^-(Ln - Lm)t\n",
    "            row[\"135/133m\"] = previous_row[\"135/133m\"] * np.exp(-(l135-l133m)*t)\n",
    "            row[\"135/133\"] = previous_row[\"135/133\"] * np.exp(-(l135-l133)*t)\n",
    "            row[\"135/131m\"] = previous_row[\"135/131m\"] * np.exp(-(l135-l131m)*t)\n",
    "            row[\"133m/133\"] = previous_row[\"133m/133\"] * np.exp(-(l133m-l133)*t)\n",
    "            row[\"133m/131m\"] = previous_row[\"133m/131m\"] * np.exp(-(l133m-l131m)*t)\n",
    "            row[\"133/131m\"] = previous_row[\"133/131m\"] * np.exp(-(l133-l131m)*t)\n",
    "    \n",
    "            if t % 9 == 0: #first isotopes released after first cycle\n",
    "                #multiply each by random variable\n",
    "                row[\"135/133m\"]  += 607\n",
    "                row[\"135/133\"]   += 66.4\n",
    "                row[\"135/131m\"]  += 220000\n",
    "                row[\"133m/133\"]  += 0.109\n",
    "                row[\"133m/131m\"] += 363\n",
    "                row[\"133/131m\"]  += 3320\n",
    "\n",
    "        isotope_rows.append(row)  \n",
    "        \n",
    "    isotope_columns = []\n",
    "    \n",
    "    for t in range(144): \n",
    "        for key, value in isotope_rows[t].items(): \n",
    "            isotope_columns.append({f'{key}_t{t}': value})\n",
    "\n",
    "    isotope_df = pd.DataFrame(isotope_columns)\n",
    "    long_row = isotope_df.sum().to_frame().T\n",
    "    \n",
    "    return long_row\n",
    " \n",
    "test = isotope_signal()\n",
    "print(test)\n",
    "\n",
    "\n",
    "#turn the isotope signal into a 1 row by 6*timestep column df that can be appended to trucks df \n",
    "# isotope_columns = []\n",
    "# for t in range(144): \n",
    "#     for key, value in test[t].items(): \n",
    "#         isotope_columns.append({f'{key}_t{t}': value})\n",
    "        \n",
    "# print(len(isotope_columns))\n",
    "# #print(isotope_columns)\n",
    "# isotope_df = pd.DataFrame(isotope_columns)\n",
    "# # isotope_df = pd.Series(isotope_columns)\n",
    "# sums = isotope_df.sum().to_frame().T\n",
    "# print(isotope_df)\n",
    "# print(sums)                                           \n",
    "# print(type(sums))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cols(max_time):\n",
    "    col_names = [\"diversion\"]\n",
    "    #edit so that transactions include all but the heu going to the heusink \n",
    "#     transactions = {(\"UraniumMine\", \"enrichment\"), (\"enrichment\", \"reactor\"), \n",
    "#                     (\"reactor\", \"SpentFuelSink\"), }#(\"enrichment\", \"HEUSink\")}\n",
    "    #[\"mine--enrich\", \"enrich--reactor\", \"enrich--heusink\", \"reactor--sfsink\"]\n",
    "#         ('milling', 'conversion'),('mil_enrichment', 'mil_str_u_dep'),('civ_enrichment', 'civ_str_u_dep'), \n",
    "#         ('conversion', 'mil_uox_fabrication'), ('civ_enrichment', 'civ_fabrication'), \n",
    "#         ('mine', 'milling'), ('conversion', 'civ_enrichment'), ('mil_enrichment', 'mil_str_fiss'), \n",
    "#         ('conversion', 'mil_enrichment')\n",
    "    transactions = {\n",
    "          ('civ_enrichment', 'civ_str_u_dep'), ('mine', 'milling'), ('milling', 'conversion'), \n",
    "          ('civ_enrichment', 'civ_fabrication'), ('conversion', 'civ_enrichment')\n",
    "    }\n",
    "    \n",
    "    for t in range(max_time):\n",
    "        for trans in sorted(transactions): \n",
    "            col_names.append(trans[0] + \"--\" + trans[1] + \"|time\" + str(t))\n",
    "    return col_names\n",
    " \n",
    "def make_row(truckdf, max_time):\n",
    "    long_row = []\n",
    "    #long_row.append(\"HEUSink\" in truckdf[\"ReceiverId\"].tolist())\n",
    "    long_row.append(\"mil_enrichment\" in truckdf[\"ReceiverId\"].tolist())\n",
    "    # mil_enrichment\n",
    "#         ('milling', 'conversion'),('mil_enrichment', 'mil_str_u_dep'),('civ_enrichment', 'civ_str_u_dep'), \n",
    "#         ('conversion', 'mil_uox_fabrication'), ('civ_enrichment', 'civ_fabrication'), \n",
    "#         ('mine', 'milling'), ('conversion', 'civ_enrichment'), ('mil_enrichment', 'mil_str_fiss'), \n",
    "#         ('conversion', 'mil_enrichment')\n",
    "    transactions = {\n",
    "        #(\"UraniumMine\", \"enrichment\"), (\"enrichment\", \"reactor\"), \n",
    "                    #(\"reactor\", \"SpentFuelSink\"), #(\"enrichment\", \"HEUSink\")\n",
    "           ('civ_enrichment', 'civ_str_u_dep'), ('mine', 'milling'), ('milling', 'conversion'), \n",
    "           ('civ_enrichment', 'civ_fabrication'), ('conversion', 'civ_enrichment')\n",
    "    }\n",
    "    sorted_trans = sorted(transactions)\n",
    "    for t in range(max_time):\n",
    "        #subset rows with this timestep\n",
    "        subset = truckdf.loc[truckdf['Time'] == t]\n",
    "        sub_row = [0] * len(transactions)\n",
    "        \n",
    "        for index, row in subset.iterrows():\n",
    "            #check each possible transaction\n",
    "            for t in range(len(transactions)): \n",
    "                if row[\"SenderId\"] == sorted_trans[t][0] and row[\"ReceiverId\"] == sorted_trans[t][1]:    \n",
    "                    sub_row[t] = truckdf.loc[index, \"truck\"]\n",
    "#             if row[\"SenderId\"] == \"UraniumMine\" and row[\"ReceiverId\"] == \"enrichment\":\n",
    "#                 #sub_row[0] = (\"mine--enrich\") #switch to the number of trucks sent!\n",
    "#                 sub_row[0] = truckdf.loc[index, \"truck\"]\n",
    "                \n",
    "#             if row[\"SenderId\"] == \"enrichment\" and row[\"ReceiverId\"] == \"reactor\":\n",
    "#                 sub_row[1] = truckdf.loc[index, \"truck\"]\n",
    "\n",
    "#             if row[\"SenderId\"] == \"enrichment\" and row[\"ReceiverId\"] == \"HEUSink\":\n",
    "#                 sub_row[2] = truckdf.loc[index, \"truck\"]\n",
    "\n",
    "#             if row[\"SenderId\"] == \"reactor\" and row[\"ReceiverId\"] == \"SpentFuelSink\":\n",
    "#                 sub_row[3] = truckdf.loc[index, \"truck\"]\n",
    "        long_row.extend(sub_row)\n",
    "    return long_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_line(filename, truck_size, max_time):\n",
    "    return make_row(send_trucks(trim_data(tidy_df(filename)), truck_size), max_time)\n",
    "    \n",
    "def simulation_data(files, truck_size, max_time):\n",
    "    columns = make_cols(max_time)\n",
    "    rows = []\n",
    "    for file in files: \n",
    "        rows.append(file_to_line(file, truck_size, max_time))\n",
    "    #return pd.DataFrame(data = rows, columns = columns)\n",
    "    return pd.concat([pd.DataFrame(data = rows, columns = columns), isotope_signal()], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-527cbc2df5fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimulation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"0309base.sqlite\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"0309reactor4.sqlite\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-73-9527fef7f191>\u001b[0m in \u001b[0;36msimulation_data\u001b[0;34m(files, truck_size, max_time)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mrows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_to_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruck_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m#return pd.DataFrame(data = rows, columns = columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misotope_signal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-9527fef7f191>\u001b[0m in \u001b[0;36mfile_to_line\u001b[0;34m(filename, truck_size, max_time)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfile_to_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruck_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmake_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msend_trucks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrim_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtidy_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruck_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimulation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruck_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-e2670842e0a6>\u001b[0m in \u001b[0;36mtidy_df\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtidy_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdbopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0magentTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AgentEntry\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cym' is not defined"
     ]
    }
   ],
   "source": [
    "simulation_data([\"0309base.sqlite\", \"0309reactor4.sqlite\"], 12000, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['swu_cycle_variance/out-run290.py-.sqlite', 'swu_cycle_variance/out-run868.py-.sqlite', 'swu_cycle_variance/out-run384.py-.sqlite', 'swu_cycle_variance/out-run49.py-.sqlite', 'swu_cycle_variance/out-run590.py-.sqlite', 'swu_cycle_variance/out-run929.py-.sqlite', 'swu_cycle_variance/out-run484.py-.sqlite', 'swu_cycle_variance/out-run153.py-.sqlite', 'swu_cycle_variance/out-run631.py-.sqlite', 'swu_cycle_variance/out-run319.py-.sqlite']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'make_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-fe7ee7f93dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbig_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m144\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#smaller_trucks = simulation_data(files[0:], 10000, 144)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-9527fef7f191>\u001b[0m in \u001b[0;36msimulation_data\u001b[0;34m(files, truck_size, max_time)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimulation_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruck_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_cols' is not defined"
     ]
    }
   ],
   "source": [
    "#big_df = simulation_data(files[0:], 20000, 144)\n",
    "#small_df = simulation_data(files[0:20], 20000, 144)\n",
    "#med_df = simulation_data(files[0:100], 20000, 144)\n",
    "#tweak = simulation_data(files[0:], 20000, 144)\n",
    "#simple = simulation_data(files[0:], 20, 144)\n",
    "\n",
    "import os\n",
    "files = []\n",
    "for file in os.listdir(\"swu_cycle_variance\"):\n",
    "    if file.endswith(\".sqlite\"):\n",
    "        #print(file)\n",
    "        #files.append(os.path.join(\"swu_cycle_variance\", file))\n",
    "        files.append(os.path.join(\"swu_cycle_variance\", file))\n",
    "\n",
    "print(files[0:10])\n",
    "big_df = simulation_data(files[0:], 20000, 144)\n",
    "#smaller_trucks = simulation_data(files[0:], 10000, 144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(big_df.shape)\n",
    "big_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple.fillna(value = 0, axis=1, inplace = True)\n",
    "simple.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(simple[\"diversion\"]))\n",
    "simple.describe()\n",
    "\n",
    "#simple.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split into training and test sets\n",
    "X = simple.loc[:, simple.columns != \"diversion\"]\n",
    "y = simple[\"diversion\"]\n",
    "trucks_train, trucks_test, diversion_train, diversion_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "s_trucks_train, s_trucks_test, diversion_train, diversion_test = train_test_split(X_scaled, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=8)\n",
    "rf.fit(s_trucks_train, diversion_train)\n",
    "\n",
    "rfy_pred = rf.predict(s_trucks_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#check performance\n",
    "print(\"Random forest accuracy:\",metrics.accuracy_score(diversion_test, rfy_pred))\n",
    "\n",
    "features = pd.Series(rf.feature_importances_,index=simple.columns[1:]).sort_values(ascending=False)\n",
    "#print([f for f in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "linsvc = LinearSVC(C=1)\n",
    "linsvc.fit(s_trucks_train, diversion_train)\n",
    "linsvy_pred = linsvc.predict(s_trucks_test)\n",
    "\n",
    "print(\"Linear SVC accuracy:\", metrics.accuracy_score(diversion_test, linsvy_pred))\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "svc = svm.SVC()\n",
    "svc.fit(s_trucks_train, diversion_train)\n",
    "svc_yhat = svc.predict(s_trucks_test)\n",
    "\n",
    "print(\"SVC accuracy:\", metrics.accuracy_score(diversion_test, svc_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver = \"sag\")\n",
    "lrmod = lr.fit(s_trucks_train, diversion_train)\n",
    "lryhat = lr.predict(s_trucks_test)\n",
    "\n",
    "print(\"Logistic regression accuracy:\", metrics.accuracy_score(diversion_test, lryhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors \n",
    "knn = neighbors.KNeighborsClassifier(3, weights = 'uniform')   \n",
    "model = knn.fit(s_trucks_train, diversion_train)\n",
    "print(model.score(s_trucks_train, diversion_train))\n",
    "\n",
    "knnhat = model.predict(s_trucks_test)\n",
    "print(\"KNN test accuracy:\", metrics.accuracy_score(diversion_test, knnhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what if it's raining? remove 30% of the data points \n",
    "X = simple.loc[:, simple.columns != \"diversion\"]\n",
    "y = simple[\"diversion\"]\n",
    "\n",
    "#go through each column, select 30% of rows in that column, change value to 0\n",
    "def mask(df, proportion):\n",
    "    new = df.copy()\n",
    "    for c in range(new.shape[1]):\n",
    "        row_ids = random.sample(range(new.shape[0]), int(new.shape[0] * proportion))\n",
    "        for row in row_ids: \n",
    "            new.iat[row, c] = 0 \n",
    "    return new\n",
    "\n",
    "X30 = mask(X, 0.3)\n",
    "print(X30.equals(X))\n",
    "\n",
    "\n",
    "trucks_train, trucks_test, diversion_train, diversion_test = train_test_split(X30, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X30_scaled = scaler.fit_transform(X30)\n",
    "sn_trucks_train, sn_trucks_test, diversion_train, diversion_test = train_test_split(X30_scaled, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = []\n",
    "for p in range(1, 8): \n",
    "    missing.append(mask(X, p * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing[0].equals(missing[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_test = []\n",
    "scaled = []\n",
    "for m in missing: \n",
    "    train_test.append(train_test_split(m, simple[\"diversion\"], test_size=0.3))\n",
    "    mscaled = scaler.fit_transform(m)\n",
    "    scaled.append(train_test_split(mscaled, simple[\"diversion\"], test_size = 0.3))\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = svm.SVC()\n",
    "svc.fit(sn_trucks_train, diversion_train)\n",
    "svc_yhat = svc.predict(sn_trucks_test)\n",
    "\n",
    "print(\"SVC accuracy:\", metrics.accuracy_score(diversion_test, svc_yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(trucks_train, diversion_train)\n",
    "\n",
    "rfy_pred = rf.predict(trucks_test)\n",
    "print(\"Random forest accuracy:\",metrics.accuracy_score(diversion_test, rfy_pred))\n",
    "print(\"Random forest auc:\",metrics.roc_auc_score(diversion_test, rfy_pred))\n",
    "print(\"Random forest confusion matrix:\", metrics.confusion_matrix(diversion_test, rfy_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(3, weights = 'uniform')   \n",
    "model = knn.fit(sn_trucks_train, diversion_train)\n",
    "print(model.score(sn_trucks_train, diversion_train))\n",
    "\n",
    "knnhat = model.predict(sn_trucks_test)\n",
    "print(\"KNN test accuracy:\", metrics.accuracy_score(diversion_test, knnhat))\n",
    "print(\"KNN auc:\", metrics.roc_auc_score(diversion_test, knnhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy = []\n",
    "rf_auc = []\n",
    "for trucks_train, trucks_test, diversion_train, diversion_test in train_test: \n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    rf.fit(trucks_train, diversion_train)\n",
    "\n",
    "    rfy_pred = rf.predict(trucks_test)\n",
    "    rf_accuracy.append(metrics.accuracy_score(diversion_test, rfy_pred))\n",
    "    rf_auc.append(metrics.roc_auc_score(diversion_test, rfy_pred))\n",
    "    print(\"Random forest accuracy:\",metrics.accuracy_score(diversion_test, rfy_pred))\n",
    "    print(\"Random forest auc:\",metrics.roc_auc_score(diversion_test, rfy_pred))\n",
    "    print(\"Random forest confusion matrix:\", metrics.confusion_matrix(diversion_test, rfy_pred))\n",
    "    print()\n",
    "    \n",
    "print(rf_accuracy)\n",
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy = []\n",
    "svc_auc = []\n",
    "knn_accuracy = []\n",
    "knn_auc = []\n",
    "for s_trucks_train, s_trucks_test, diversion_train, diversion_test in scaled:\n",
    "    svc = svm.SVC()\n",
    "    svc.fit(s_trucks_train, diversion_train)\n",
    "    svc_yhat = svc.predict(s_trucks_test)\n",
    "    svc_accuracy.append(metrics.accuracy_score(diversion_test, svc_yhat))\n",
    "    svc_auc.append(metrics.roc_auc_score(diversion_test, svc_yhat))\n",
    "    print(\"SVC accuracy:\", metrics.accuracy_score(diversion_test, svc_yhat))\n",
    "    print(\"SVC auc:\",metrics.roc_auc_score(diversion_test, svc_yhat))\n",
    "    print(\"SVC confusion matrix:\", metrics.confusion_matrix(diversion_test, svc_yhat))\n",
    "    print()\n",
    "    \n",
    "    knn = neighbors.KNeighborsClassifier(3, weights = 'uniform')   \n",
    "    model = knn.fit(s_trucks_train, diversion_train)\n",
    "    print(model.score(s_trucks_train, diversion_train))\n",
    "\n",
    "    knnhat = model.predict(s_trucks_test)\n",
    "    knn_accuracy.append(metrics.accuracy_score(diversion_test, knnhat))\n",
    "    knn_auc.append(metrics.roc_auc_score(diversion_test, knnhat))\n",
    "    print(\"KNN test accuracy:\", metrics.accuracy_score(diversion_test, knnhat))\n",
    "    print(\"KNN auc:\",metrics.roc_auc_score(diversion_test, knnhat))\n",
    "    print(\"KNN confusion matrix:\", metrics.confusion_matrix(diversion_test, knnhat))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for anna's model: \n",
    "\n",
    "print(svc_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "#percent of data missing from the set on x axis, accuracy on y axis\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], rf_accuracy, marker='o', color='b', alpha=0.7,\n",
    "            s = 124, label='Random Forest Classifier')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], svc_accuracy, marker='o', color='r', alpha=0.7, \n",
    "            s = 124, label='Support Vector Classifier')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], knn_accuracy, marker='o', color='g', alpha=0.7, \n",
    "            s = 124, label='K-Nearest Neighbors Classifier')\n",
    "plt.title(\"Accuracy of Classifiers for Schema A\")\n",
    "plt.xlabel(\"Percent of Data Excluded from Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0.8, 1.05)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "#percent of data missing from the set on x axis, accuracy on y axis\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], rf_auc, marker='o', color='b', alpha=0.7,\n",
    "            s = 124, label='Random Forest AUC')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], svc_auc, marker='o', color='r', alpha=0.7, \n",
    "            s = 124, label='SVC AUC')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], knn_auc, marker='o', color='g', alpha=0.7, \n",
    "            s = 124, label='KNN AUC')\n",
    "plt.title(\"AUC of Classifiers for Schema A\")\n",
    "plt.xlabel(\"Percent of Data Excluded from Model\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0.8, 1.05)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for baptiste's model: \n",
    "#use big_df\n",
    "X = big_df.loc[:, big_df.columns != \"diversion\"]\n",
    "y = big_df[\"diversion\"]\n",
    "trucks_train, trucks_test, diversion_train, diversion_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "missing = []\n",
    "for p in range(1, 8): \n",
    "    missing.append(mask(X, p * 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train_test = []\n",
    "scaled = []\n",
    "for m in missing: \n",
    "    train_test.append(train_test_split(m, big_df[\"diversion\"], test_size=0.3))\n",
    "    mscaled = scaler.fit_transform(m)\n",
    "    scaled.append(train_test_split(mscaled, big_df[\"diversion\"], test_size = 0.3))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_accuracy = []\n",
    "rf_auc = []\n",
    "for trucks_train, trucks_test, diversion_train, diversion_test in train_test: \n",
    "    rf = RandomForestClassifier(n_estimators=3)\n",
    "    rf.fit(trucks_train, diversion_train)\n",
    "\n",
    "    rfy_pred = rf.predict(trucks_test)\n",
    "    rf_accuracy.append(metrics.accuracy_score(diversion_test, rfy_pred))\n",
    "    rf_auc.append(metrics.roc_auc_score(diversion_test, rfy_pred))\n",
    "    print(\"Random forest accuracy:\",metrics.accuracy_score(diversion_test, rfy_pred))\n",
    "    print(\"Random forest auc:\",metrics.roc_auc_score(diversion_test, rfy_pred))\n",
    "    print(\"Random forest confusion matrix:\", metrics.confusion_matrix(diversion_test, rfy_pred))\n",
    "    print()\n",
    "    \n",
    "print(rf_accuracy)\n",
    "print(rf_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_accuracy = []\n",
    "svc_auc = []\n",
    "knn_accuracy = []\n",
    "knn_auc = []\n",
    "for s_trucks_train, s_trucks_test, diversion_train, diversion_test in scaled:\n",
    "    svc = svm.SVC()\n",
    "    svc.fit(s_trucks_train, diversion_train)\n",
    "    svc_yhat = svc.predict(s_trucks_test)\n",
    "    svc_accuracy.append(metrics.accuracy_score(diversion_test, svc_yhat))\n",
    "    svc_auc.append(metrics.roc_auc_score(diversion_test, svc_yhat))\n",
    "    print(\"SVC accuracy:\", metrics.accuracy_score(diversion_test, svc_yhat))\n",
    "    print(\"SVC auc:\",metrics.roc_auc_score(diversion_test, svc_yhat))\n",
    "    print(\"SVC confusion matrix:\", metrics.confusion_matrix(diversion_test, svc_yhat))\n",
    "    print()\n",
    "    \n",
    "    knn = neighbors.KNeighborsClassifier(3, weights = 'uniform')   \n",
    "    model = knn.fit(s_trucks_train, diversion_train)\n",
    "    print(model.score(s_trucks_train, diversion_train))\n",
    "\n",
    "    knnhat = model.predict(s_trucks_test)\n",
    "    knn_accuracy.append(metrics.accuracy_score(diversion_test, knnhat))\n",
    "    knn_auc.append(metrics.roc_auc_score(diversion_test, knnhat))\n",
    "    print(\"KNN test accuracy:\", metrics.accuracy_score(diversion_test, knnhat))\n",
    "    print(\"KNN auc:\",metrics.roc_auc_score(diversion_test, knnhat))\n",
    "    print(\"KNN confusion matrix:\", metrics.confusion_matrix(diversion_test, knnhat))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn_accuracy)\n",
    "print(svc_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "#percent of data missing from the set on x axis, accuracy on y axis\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], rf_accuracy, marker='o', color='b', alpha=0.7,\n",
    "            s = 124, label='Random Forest Classifier')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], svc_accuracy, marker='o', color='r', alpha=0.7, \n",
    "            s = 124, label='Support Vector Classifier')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], knn_accuracy, marker='o', color='g', alpha=0.7, \n",
    "            s = 124, label='K-Nearest Neighbors Classifier')\n",
    "plt.title(\"Accuracy of Classifiers for Schema B\")\n",
    "plt.xlabel(\"Percent of Data Excluded from Model\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0.75, 1.05)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "#percent of data missing from the set on x axis, accuracy on y axis\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], rf_auc, marker='o', color='b', alpha=0.7,\n",
    "            s = 124, label='Random Forest AUC')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], svc_auc, marker='o', color='r', alpha=0.7, \n",
    "            s = 124, label='SVC AUC')\n",
    "\n",
    "plt.scatter([10, 20, 30, 40, 50, 60, 70], knn_auc, marker='o', color='g', alpha=0.7, \n",
    "            s = 124, label='KNN AUC')\n",
    "plt.title(\"AUC of Classifiers for Schema B\")\n",
    "plt.xlabel(\"Percent of Data Excluded from Model\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.xlim(0, 80)\n",
    "plt.ylim(0.75, 1.05)\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
